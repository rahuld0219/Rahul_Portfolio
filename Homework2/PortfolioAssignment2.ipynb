{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NLTK\n",
    "### Rahul Das, CS 4395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to access text samples\n",
    "from nltk.book import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "In NLTK's Text object, the data of a text is stored as **tokens**. Each token is a unit that the text is broken down into, the smallest units being words and punctuation. By using the built in *tokens* method we can see the tokens from a specific text object. The tokens are returned in a list format.\n",
    "\n",
    "Here we save and display the first 20 tokens of the returned token list for text1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar']\n"
     ]
    }
   ],
   "source": [
    "toks = text1.tokens[:20]\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "NLTK offers a way of viewing a text called concordance that can be very helpful. A concordance of a certain word shows every occurrence of the word as well as some context around it from a certain Text object.\n",
    "\n",
    "Here we view a concordance for 5 occurrences of the word 'sea' in text1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 455 matches:\n",
      " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
      " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
      "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
      "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
      " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(text1.concordance('sea', lines = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count\n",
    "The NLTK Text object has its own count function which is slightly different from the default python count method. The key difference here is that the NLTK count returns the number of occurences of a word within a Text object, whereas the default python count finds the number of occurrences of a specific element in a list or a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurences of 'sea' in Moby Dick:  433\n",
      "Occurences of 'Sea' in Moby Dick:  22\n",
      "Occurrences of 't' in the sentence:  2\n",
      "Occurrences of 'T' in the sentence:  1\n",
      "Occurrences of 'a' in the list:  3\n",
      "Occurrences of 15 in the list:  2\n"
     ]
    }
   ],
   "source": [
    "# NLTK count\n",
    "print('Occurences of \\'sea\\' in Moby Dick: ', text1.count('sea'))\n",
    "# count is case sensitive\n",
    "print('Occurences of \\'Sea\\' in Moby Dick: ', text1.count('Sea'))\n",
    "\n",
    "# Python default count\n",
    "# String - lowercase count should be 2, uppercase should be 1\n",
    "sentence = \"The quick brown fox jumps over the lazy dog twice.\"\n",
    "print('Occurrences of \\'t\\' in the sentence: ', sentence.count('t'))\n",
    "print('Occurrences of \\'T\\' in the sentence: ', sentence.count('T'))\n",
    "# List - 'a': 3, 15: 2\n",
    "data = ['a', 'b', 'a', 'a', 'c', 5, 10, 10, 15, 10, 10, 15]\n",
    "print('Occurrences of \\'a\\' in the list: ', data.count('a'))\n",
    "# works for any element of a list regardless of data type\n",
    "print('Occurrences of 15 in the list: ', data.count(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "While NLTK does provide a few pre-packaged text samples, it is important for us to be able to process our own texts. In order to store a text into a Text object, it must first be turned into a list of tokens to be stored. This can be done with the word_tokenize function. This function is able to tokenize incoming text into word units. An alternative approach is tokenizing into sentences using the sent_tokenize method.\n",
    "\n",
    "To demonstrate the tokenization process, I am going to tokenize an excerpt from The Last Olympian by Rick Riordan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use word_tokenize and sent_tokenize\n",
    "from nltk.tokenize import *\n",
    "# Defines text sample to be used by tokenizers\n",
    "raw_text = 'The main courtyard was filled with warriors - mermen with fish tails from the waist down and human bodies from the waist up, except their skin was blue, which I\\'d never known before. Some were tending to the wounded. Some were sharpening spears and swords. One passed us, swimming in a hurry. His eyes were bright green, like that stuff they put in glo-sticks, and his teeth were shark teeth. They don\\'t show you stuff like that in The Little Mermaid.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'main', 'courtyard', 'was', 'filled', 'with', 'warriors', '-', 'mermen', 'with', 'fish', 'tails', 'from', 'the', 'waist', 'down', 'and', 'human', 'bodies', 'from', 'the', 'waist', 'up', ',', 'except', 'their', 'skin', 'was', 'blue', ',', 'which', 'I', \"'d\", 'never', 'known', 'before', '.', 'Some', 'were', 'tending', 'to', 'the', 'wounded', '.']\n"
     ]
    }
   ],
   "source": [
    "wordTokens = word_tokenize(raw_text)\n",
    "# only displays first 44 tokens of the text in order to prove the words were tokenized\n",
    "print(wordTokens[:44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The main courtyard was filled with warriors - mermen with fish tails from the waist down and human bodies from the waist up, except their skin was blue, which I'd never known before.\", 'Some were tending to the wounded.', 'Some were sharpening spears and swords.', 'One passed us, swimming in a hurry.', 'His eyes were bright green, like that stuff they put in glo-sticks, and his teeth were shark teeth.', \"They don't show you stuff like that in The Little Mermaid.\"]\n"
     ]
    }
   ],
   "source": [
    "sentTokens = sent_tokenize(raw_text)\n",
    "# prints all tokenized sentences\n",
    "print(sentTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stems and Lemma\n",
    "Stems in text are a form of the word with their prefixes and suffixes removed. This often leaves us with sequences that aren't necessarily actual words, but have all of the additional prefixes and suffixes removed. These stemmed \"words\" aren't always successfully useful, but on some occasions they can remove parts of the word that are not necessary for tone or meaning.\n",
    "\n",
    "Lemma, on the other hand are the base form of a word. They revert given words back to their most basic dictionary form. This can overcome the problems that come with stemming where stems end up sometimes being incoherent or useless. By checking against a dictionary to find an actual base form word, you ensure that meaning is preserved. This however has the drawback of taking more time to search a detailed dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use stemmer and lemmatizer\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'main', 'courtyard', 'wa', 'fill', 'with', 'warrior', '-', 'mermen', 'with', 'fish', 'tail', 'from', 'the', 'waist', 'down', 'and', 'human', 'bodi', 'from', 'the', 'waist', 'up', ',', 'except', 'their', 'skin', 'wa', 'blue', ',', 'which', 'i', \"'d\", 'never', 'known', 'befor', '.', 'some', 'were', 'tend', 'to', 'the', 'wound', '.', 'some', 'were', 'sharpen', 'spear', 'and', 'sword', '.', 'one', 'pass', 'us', ',', 'swim', 'in', 'a', 'hurri', '.', 'hi', 'eye', 'were', 'bright', 'green', ',', 'like', 'that', 'stuff', 'they', 'put', 'in', 'glo-stick', ',', 'and', 'hi', 'teeth', 'were', 'shark', 'teeth', '.', 'they', 'do', \"n't\", 'show', 'you', 'stuff', 'like', 'that', 'in', 'the', 'littl', 'mermaid', '.']\n"
     ]
    }
   ],
   "source": [
    "# Creates a stemmer that uses the Proter algorithm for stemming\n",
    "stemmer = PorterStemmer()\n",
    "# List comprehension that goes through and applies stemmer to every element of the word tokens list\n",
    "stems = [stemmer.stem(currTok) for currTok in wordTokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'main', 'courtyard', 'wa', 'filled', 'with', 'warrior', '-', 'merman', 'with', 'fish', 'tail', 'from', 'the', 'waist', 'down', 'and', 'human', 'body', 'from', 'the', 'waist', 'up', ',', 'except', 'their', 'skin', 'wa', 'blue', ',', 'which', 'I', \"'d\", 'never', 'known', 'before', '.', 'Some', 'were', 'tending', 'to', 'the', 'wounded', '.', 'Some', 'were', 'sharpening', 'spear', 'and', 'sword', '.', 'One', 'passed', 'u', ',', 'swimming', 'in', 'a', 'hurry', '.', 'His', 'eye', 'were', 'bright', 'green', ',', 'like', 'that', 'stuff', 'they', 'put', 'in', 'glo-sticks', ',', 'and', 'his', 'teeth', 'were', 'shark', 'teeth', '.', 'They', 'do', \"n't\", 'show', 'you', 'stuff', 'like', 'that', 'in', 'The', 'Little', 'Mermaid', '.']\n"
     ]
    }
   ],
   "source": [
    "# Creates a lemmatizer using the WordNet approach to lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# List comprehension that goes through and applies lemmatizer to every element of the word tokens list\n",
    "lemma = [lemmatizer.lemmatize(currTok) for currTok in wordTokens]\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some differences between the stems and lemmas\n",
    "*(In the form stem - lemma)*\n",
    "- fill - filled\n",
    "- mermen - merman\n",
    "- bodi - body\n",
    "- tend - tending\n",
    "- hurri - hurry\n",
    "- sharpen - sharpening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions About NLTK\n",
    "### Functionality\n",
    "I think the NLTK is a very powerful and incredibly useful tool for language processing. People tend to underestimate the complexity and nuance of human language, so processing language may seem easy at first but it can become an incredibly daunting task for a programmer. The NLTK library accounts for this complexity and offers several powerful tooks to help programmers begin processing natural language texts. I think this is a great tool and I look forward to using it more as I explore natural language processing.\n",
    "### Code Quality\n",
    "The NLTK documentation and syntax is some of the cleanest I've seen of the libraries I've used over the years. Many powerful libraries fall into the error of poorly maintaining their documentation, or overcomplicating their tools. I believe NLTK has done a very good job at managing both. The useability and code quality of NLTK is very high and intuitive. The documentation is very clean and informative. I am looking forward to writing code using NLTK because of its high quality.\n",
    "### Potential Future Uses\n",
    "NLTK provides many of the tools that allow computers to interact with language. This along with applications of machine learning can make some very powerful programs such as spam classification, simple chatbots, search algorithms, and other similar tasks that require an overlap of processing and understanding language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
