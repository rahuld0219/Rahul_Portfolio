{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Author Attribution**\n",
    "\n",
    "Different machine learning models are better suited for different tasks. This Notebook explores the performance of different ML methods for a classification task on bodies of text. The data for testing and training the models comes from `federalist.csv`. The classification labels are the authors: Alexander Hamilton, James Madison, and John Jay. The Data is made up of 83 documents from the Federalist Papers. This program uses ML to classify the authorship of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text preprocessing\n",
    "import pandas as pd\n",
    "# To split the given dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# To Vectorize the bodies of text to allow model fitting\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Machine Learning Models\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# For evaluating model performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "# For text preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "# To catch ConvergenceWarning from the neural network\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     author                                               text\n",
      "0  HAMILTON  FEDERALIST. No. 1 General Introduction For the...\n",
      "1       JAY  FEDERALIST No. 2 Concerning Dangers from Forei...\n",
      "2       JAY  FEDERALIST No. 3 The Same Subject Continued (C...\n",
      "3       JAY  FEDERALIST No. 4 The Same Subject Continued (C...\n",
      "4       JAY  FEDERALIST No. 5 The Same Subject Continued (C...\n",
      "\n",
      "Counts by Author\n",
      "___________________________\n",
      "HAMILTON                49\n",
      "MADISON                 15\n",
      "HAMILTON OR MADISON     11\n",
      "JAY                      5\n",
      "HAMILTON AND MADISON     3\n",
      "Name: author, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# PART 1\n",
    "\n",
    "data = pd.read_csv('federalist.csv')\n",
    "#Sets the values of the author column to category type\n",
    "data.author = data.author.astype('category')\n",
    "\n",
    "# splits dataframe into labels and data\n",
    "Y = data.author\n",
    "X = data.text\n",
    "\n",
    "#prints First few columns \\n Counts of authors\n",
    "print(data.head())\n",
    "print('\\nCounts by Author\\n___________________________')\n",
    "print(Y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape:\t (66,)\n",
      "Testing Dataset Shape:\t (17,)\n"
     ]
    }
   ],
   "source": [
    "# PART 2\n",
    "\n",
    "# Splits data into 80% training, 20% testing\n",
    "rawTrainX, rawTestX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
    "print('Training Dataset Shape:\\t', rawTrainX.shape)\n",
    "print('Testing Dataset Shape:\\t', rawTestX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "Since machine learning models are just complex mathematical models under the hood, text data must be processed into meaningful numerical representations. To do this, TF-IDF Vectorization can be used. TF-IDF stands for *term frequncy-inverse document frequency*. This is a metric that reflects how \"important\" a word is in the context of a document. A higher value for a word reflects a higher occurrence/importance in the text body. These values are used as weighting factors for the classification model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape:\t (66, 7876)\n",
      "Testing Set Shape:\t (17, 7876)\n"
     ]
    }
   ],
   "source": [
    "# PART 3\n",
    "\n",
    "# Vectorizer processes text while ignoring the nltk stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "vect = TfidfVectorizer(stop_words=sw)\n",
    "\n",
    "# Processed data sets\n",
    "trainX = vect.fit_transform(rawTrainX)\n",
    "testX = vect.transform(rawTestX)\n",
    "\n",
    "print('Training Set Shape:\\t', trainX.shape)\n",
    "print('Testing Set Shape:\\t', testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes Classifier\n",
    "\n",
    "**Naive Bayes** Classifiers are simple classifiers that use a statistical model that makes the \"naive\" assumption of independence between data features. The Naive Bayes probabilistic model uses Bayes' Theorem by observing probabilities from prior data observations to make a prediction of the most likely classification of a given data point.\n",
    "\n",
    "Here, `sklearn`'s default BernoulliNB classifier model is fitted to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\t58.824%\n"
     ]
    }
   ],
   "source": [
    "# PART 4\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(trainX, trainY)\n",
    "\n",
    "preds = bnb.predict(testX)\n",
    "\n",
    "print('Accuracy Score:\\t%.3f%%' % (accuracy_score(testY, preds) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above accuracy is not very high, but it is possible to adjust the classifier to better fit the training data. By decreasing the number of features that are fitted to prevent overfitting, and including bigrams to get more context to the model, we can help the classifier model better fit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Training Set Shape:\t (66, 1000)\n",
      "Updated Testing Set Shape:\t (17, 1000)\n",
      "\n",
      "Updated Accuracy Score:\t94.118%\n"
     ]
    }
   ],
   "source": [
    "# PART 5\n",
    "\n",
    "maxFeats = 1000\n",
    "newVect = TfidfVectorizer(stop_words=sw, max_features=maxFeats, ngram_range=(1,2))\n",
    "trainX = newVect.fit_transform(rawTrainX)\n",
    "testX = newVect.transform(rawTestX)\n",
    "\n",
    "print('Updated Training Set Shape:\\t', trainX.shape)\n",
    "print('Updated Testing Set Shape:\\t', testX.shape)\n",
    "\n",
    "bnb.fit(trainX, trainY)\n",
    "\n",
    "preds = bnb.predict(testX)\n",
    "\n",
    "print('\\nUpdated Accuracy Score:\\t%.3f%%' % (accuracy_score(testY, preds) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier\n",
    "\n",
    "The **Logistic Regression** Classifier creates a linear decision boundary using a linear combination of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\t58.824%\n"
     ]
    }
   ],
   "source": [
    "# PART 6\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(trainX, trainY)\n",
    "\n",
    "preds = lr.predict(testX)\n",
    "\n",
    "print('Accuracy Score:\\t%.3f%%' % (accuracy_score(testY, preds) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above accuracy is, once again, not very high, but it is possible to adjust the classifier to better fit the training data. The C parameter of the Regression model is the inverse of the regularization strength. A higher C value indicates that the model will give a higher weight to the training date. By increasing the C value of the model from the default value of 1, we can lead the model to \"trust\" the training data more and fit the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Accuracy Score:\t76.471%\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=30)\n",
    "lr.fit(trainX, trainY)\n",
    "\n",
    "preds = lr.predict(testX)\n",
    "\n",
    "print('Updated Accuracy Score:\\t%.3f%%' % (accuracy_score(testY, preds) * 100))\n",
    "# print(lr.score(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier\n",
    "\n",
    "**Neural Networks** are machine learning  models composed of \"layers\" of nodes that each have a weight and threshold. Each node contributes to the final decision of the Network, with the role it plays determined by the weight of the node. With the combined decisions of all of the nodes, the Network outputs the prediction.\n",
    "\n",
    "Below, to find the best topology of a 2 layer Neural Network, all possible combinations of topologies are tested. It's possible that more layers can be more accurate but will take longer to find by using this method. This is a naive approach and could possibly be more efficient, but this works well enough for finding a good, simple 2 layer topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score:\t88.235% with layers: 7, 11\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "maxAcc, maxFirst, maxSecond = 0, 0, 0\n",
    "\n",
    "for i in range(1,15):\n",
    "    for j in range(1,15):\n",
    "        try:\n",
    "            nn = MLPClassifier(hidden_layer_sizes=(i,j), max_iter=500, random_state=1)\n",
    "\n",
    "            nn.fit(trainX, trainY)\n",
    "\n",
    "            preds = nn.predict(testX)\n",
    "\n",
    "            currAcc = accuracy_score(testY, preds)\n",
    "            if currAcc > maxAcc:\n",
    "                maxAcc = currAcc\n",
    "                maxFirst = i\n",
    "                maxSecond = j\n",
    "        except:\n",
    "            errors.append((i, j))\n",
    "\n",
    "print('Best Accuracy Score:\\t%.3f%% with layers: %d, %d' % ((maxAcc * 100), maxFirst, maxSecond))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
