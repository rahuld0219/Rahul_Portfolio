{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WordNet**\n",
    "\n",
    "WordNet is a large database of English words that are organized into various relational groupings. Words can be grouped together into sets of cognitive synonyms called synsets. Additionally, they can be hierarchically grouped in terms of other syntactical relations such as antonyms, hyponyms, hypernyms, meronyms, and holonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows WordNet and SentiWordNet functions to be run\n",
    "from nltk.corpus import wordnet as wn, sentiwordnet as swn\n",
    "# For Lesk Algorithm demonstration\n",
    "from nltk.wsd import lesk\n",
    "# For Collocations\n",
    "import math\n",
    "from nltk.book import text4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *WordNet Applied to a Noun*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing All Synsets\n",
    "\n",
    "`synsets(word, pos)`\n",
    "\n",
    "`word`: String value - Word to return synsets for\n",
    "\n",
    "`pos`: *(Optional)* String - Part of speech constraint\n",
    "\n",
    "*returns*: List of Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# Noun in a variable to easily run the code for other nouns\n",
    "noun = 'dog'\n",
    "# Part of speech specified to be a noun\n",
    "nounSynsets = wn.synsets(noun, pos=wn.NOUN)\n",
    "print(nounSynsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a Synset's Information\n",
    "\n",
    "`name()`: *returns* String\n",
    "\n",
    "`definition()`: *returns* String\n",
    "\n",
    "`examples()`: *returns* List of Strings\n",
    "\n",
    "`lemmas()`: *returns* List of Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of dog.n.01 : a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds \n",
      "\n",
      "Usage of dog.n.01 : ['the dog barked all night'] \n",
      "\n",
      "Lemmas of dog.n.01 : [Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n"
     ]
    }
   ],
   "source": [
    "# Synset being examined\n",
    "nounSyn = nounSynsets[0]\n",
    "\n",
    "# References the list of synsets so this code will run with different choices of noun\n",
    "nounName = nounSyn.name()\n",
    "nounDef = nounSyn.definition()\n",
    "nounUsage = nounSyn.examples()\n",
    "nounLemmas = nounSyn.lemmas()\n",
    "\n",
    "print('Definition of', nounName, ':', nounDef, '\\n')\n",
    "print('Usage of', nounName, ':', nounUsage, '\\n')\n",
    "print('Lemmas of', nounName, ':', nounLemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet Hierarchy\n",
    "\n",
    "WordNet organizes synsets into a hierarchy of relations between them. This hierarchy can be traversed by following these relationships. The top of any particular noun's hierachy is `entity.n.01`. Nouns are related to one another in a more structured way in the sense that they can be traced to a hierachy with `entity.n.01` at the top.\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "**Hypernym**: A superordinate (has a broader meaning) of a word - *color* is a hypernym of *blue*\n",
    "\n",
    "**Hyponym**: A subordinate (has a more specific meaning) of a word - *hammer* is a hyponym of *tool*\n",
    "\n",
    "**Meronym**: A part of something - *toe* is a meronym of *foot*\n",
    "\n",
    "**Holonym**: The whole that something is a part of - *couch* is a holonym of *cushion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing the Hierarchy\n",
    "\n",
    "Here the common `entity.n.01` synset can be seen for two nouns. Even though they have fairly different meanings, since both `dog.n.01` and `frank.n.02` are nouns they have `entity.n.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog.n.01 : [Synset('canine.n.02'), Synset('domestic_animal.n.01'), Synset('carnivore.n.01'), Synset('animal.n.01'), Synset('placental.n.01'), Synset('organism.n.01'), Synset('mammal.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'), Synset('whole.n.02'), Synset('chordate.n.01'), Synset('object.n.01'), Synset('physical_entity.n.01'), Synset('entity.n.01')]\n",
      "\n",
      "frank.n.02 : [Synset('sausage.n.01'), Synset('meat.n.01'), Synset('food.n.02'), Synset('solid.n.01'), Synset('matter.n.03'), Synset('physical_entity.n.01'), Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# A function that traverses up a synset's hierarchy\n",
    "hyperFunc = lambda syn: syn.hypernyms()\n",
    "# closure() applies a function to a synset\n",
    "# Used here to follow the hypernym relations up the hierarchy\n",
    "nounHierarchy = list(nounSyn.closure(hyperFunc))\n",
    "\n",
    "# Another noun can be chose to demonstrate the shared entity\n",
    "altNounSyn = nounSynsets[4]\n",
    "altNounName = altNounSyn.name()\n",
    "altNounHierarchy = list(altNounSyn.closure(hyperFunc))\n",
    "\n",
    "print(nounName, ':', nounHierarchy)\n",
    "print()\n",
    "print(altNounName, ':', altNounHierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations Between Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernyms of dog.n.01 : [Synset('canine.n.02'), Synset('domestic_animal.n.01')] \n",
      "\n",
      "Hyponyms of dog.n.01 : [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')] \n",
      "\n",
      "Meronyms of dog.n.01 : [Synset('flag.n.07')] \n",
      "\n",
      "Holonyms of dog.n.01 : [] \n",
      "\n",
      "Antonyms of dog.n.01 : [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nounHypernyms = nounSynsets[0].hypernyms()\n",
    "nounHyponyms = nounSynsets[0].hyponyms()\n",
    "nounMeronyms = nounSynsets[0].part_meronyms()\n",
    "nounHolonyms = nounSynsets[0].part_holonyms()\n",
    "# Antonyms are defined only over lemmas\n",
    "nounAntonyms = nounSynsets[0].lemmas()[0].antonyms()\n",
    "\n",
    "print('Hypernyms of', nounName, ':', nounHypernyms, '\\n')\n",
    "print('Hyponyms of', nounName, ':', nounHyponyms, '\\n')\n",
    "print('Meronyms of', nounName, ':', nounMeronyms, '\\n')\n",
    "print('Holonyms of', nounName, ':', nounHolonyms, '\\n')\n",
    "print('Antonyms of', nounName, ':', nounAntonyms, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *WordNet Applied to a Verb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing All Synsets\n",
    "\n",
    "Here the part of speech will be limited to verbs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('run.v.01'), Synset('scat.v.01'), Synset('run.v.03'), Synset('operate.v.01'), Synset('run.v.05'), Synset('run.v.06'), Synset('function.v.01'), Synset('range.v.01'), Synset('campaign.v.01'), Synset('play.v.18'), Synset('run.v.11'), Synset('tend.v.01'), Synset('run.v.13'), Synset('run.v.14'), Synset('run.v.15'), Synset('run.v.16'), Synset('prevail.v.03'), Synset('run.v.18'), Synset('run.v.19'), Synset('carry.v.15'), Synset('run.v.21'), Synset('guide.v.05'), Synset('run.v.23'), Synset('run.v.24'), Synset('run.v.25'), Synset('run.v.26'), Synset('run.v.27'), Synset('run.v.28'), Synset('run.v.29'), Synset('run.v.30'), Synset('run.v.31'), Synset('run.v.32'), Synset('run.v.33'), Synset('run.v.34'), Synset('ply.v.03'), Synset('hunt.v.01'), Synset('race.v.02'), Synset('move.v.13'), Synset('melt.v.01'), Synset('ladder.v.01'), Synset('run.v.41')]\n"
     ]
    }
   ],
   "source": [
    "verb = 'run'\n",
    "verbSynsets = wn.synsets(verb, pos=wn.VERB)\n",
    "print(verbSynsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a Synset's Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of operate.v.01 : direct or control; projects, businesses, etc. \n",
      "\n",
      "Usage of operate.v.01 : ['She is running a relief operation in the Sudan'] \n",
      "\n",
      "Lemmas of operate.v.01 : [Lemma('operate.v.01.operate'), Lemma('operate.v.01.run')]\n"
     ]
    }
   ],
   "source": [
    "verbSyn = verbSynsets[3]\n",
    "\n",
    "verbName = verbSyn.name()\n",
    "verbDef = verbSyn.definition()\n",
    "verbUsage = verbSyn.examples()\n",
    "verbLemmas = verbSyn.lemmas()\n",
    "\n",
    "print('Definition of', verbName, ':', verbDef, '\\n')\n",
    "print('Usage of', verbName, ':', verbUsage, '\\n')\n",
    "print('Lemmas of', verbName, ':', verbLemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet Hierarchy\n",
    "\n",
    "Since verbs don't have `entity.n.01` as a hierarchical root, these hierarchy trees can vary greatly. Verbs are organized in a less structured hierarchy in the sense that they do not necessarily share a single hierarchy tree. This means that different verbs can have different hierarchies. In this case, `operate.v.01` has a root of `control.v.01` while `run.v.01` has a root of `travel.v.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operate.v.01 : [Synset('direct.v.04'), Synset('manage.v.02'), Synset('control.v.01')]\n",
      "\n",
      "run.v.01 : [Synset('travel_rapidly.v.01'), Synset('travel.v.01')]\n"
     ]
    }
   ],
   "source": [
    "# Uses the hyperFunc lambda function defined earlier\n",
    "verbHierarchy = list(verbSyn.closure(hyperFunc))\n",
    "\n",
    "# To examine different hierarchies\n",
    "altVerbSyn = verbSynsets[0]\n",
    "altVerbName = altVerbSyn.name()\n",
    "altVerbHierarchy = list(altVerbSyn.closure(hyperFunc))\n",
    "\n",
    "print(verbName, ':', verbHierarchy)\n",
    "print()\n",
    "print(altVerbName, ':', altVerbHierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphy\n",
    "\n",
    "Morphy allows us to do searches on inflected forms of words. This is an important function to have because WordNet usually only stores the base forms of words. Morphy applies a set of morphological rules while also checking for exceptions to handle many different possibilities of inflectional forms. This can be useful for words that can have a wide variety of inflected forms. \n",
    "\n",
    "`morphy(word, pos)`\n",
    "\n",
    "`word`: String - the inflected form of a word\n",
    "\n",
    "`pos`: *(Optional)* String - Part of speech constraint\n",
    "\n",
    "*returns*: String - The root form of the inflected word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see morphy in action using the different inflected forms of the word **\"dog\"**. It can be seen that the inflected words have different synsets associated with them. By using morphy's morphological analysis, however, it can be determined that all the different words with different meanings and seemingly different origins are all actually derived from the same root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs - dog\n",
      "dogged - dog\n",
      "dogging - dog\n",
      "\n",
      "dogs - [Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
      "dogged - [Synset('chase.v.01'), Synset('dogged.s.01')]\n",
      "dogging - [Synset('chase.v.01'), Synset('dogging.s.01')]\n"
     ]
    }
   ],
   "source": [
    "wordList = ['dogs', 'dogged', 'dogging']\n",
    "\n",
    "# Extracts and prints root word for the different inflected forms\n",
    "for word in wordList:\n",
    "    rootWord = wn.morphy(word)\n",
    "    print(word, '-', rootWord)\n",
    "\n",
    "print()\n",
    "\n",
    "# Shows the various synsets for each inflected form\n",
    "for word in wordList:\n",
    "    syns = wn.synsets(word)\n",
    "    print(word, '-', syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wu-Palmer Similarity Metric\n",
    "\n",
    "There are a few different approaches to measuring the semantic similarity of two words. WordNet provides a similarity metric called Wu-Palmer similarity. The Wu-Palmer Metric measures the similarity of two synsets by comparing their depths in the WordNet taxonomy hierarchies as well as the least distant (most specific) common parent node. \n",
    "\n",
    "`synset1.wup_similarity(synset2)`\n",
    "\n",
    "`synset1`, `synset2`: The two synsets that are being compared\n",
    "\n",
    "*returns*: A decimal that ranges from 0 to 1 (least similar to identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Metric of truck.n.01 and car.n.01 : 0.9166666666666666\n",
      "\n",
      "Similarity Metric of tiger.n.01 and cicada.n.01 : 0.6\n"
     ]
    }
   ],
   "source": [
    "# Demonstrates high metric for 2 similar synsets\n",
    "similarity1 = wn.synset('truck.n.01')\n",
    "similarity2 = wn.synset('car.n.01')\n",
    "\n",
    "# Demonstrates low metric for 2 synsets that aren't similar\n",
    "difference1 = wn.synset('tiger.n.01')\n",
    "difference2 = wn.synset('cicada.n.01')\n",
    "\n",
    "print('Similarity Metric of', similarity1.name(), 'and', similarity2.name(), ':', similarity1.wup_similarity(similarity2))\n",
    "print()\n",
    "print('Similarity Metric of', difference1.name(), 'and', difference2.name(), ':', difference1.wup_similarity(difference2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesk Algorithm\n",
    "\n",
    "Another similarity algorithm NLTK provides is an implementation of the Lesk Algorithm. The Lesk algorithm uses a context sentence to identify the synset that contains the most similarities to the target word. This algorithm works under the assumption that the context around a specific word in a sentence will likely be related to that word. It finds the highest number of words in common between the words and definitions of the target and the context sentence.\n",
    "\n",
    "`lesk(sentence, target, pos)`\n",
    "\n",
    "`sentence`: List of Strings - A list of the words in a context sentence.\n",
    "\n",
    "`target`: String value - The target word that we want to run the algorithm on.\n",
    "\n",
    "`pos`: *(Optional)* String - Part of speech constraint\n",
    "\n",
    "*returns*: Synset - The most overlapping word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context sentence below, the Lesk algorithm does a good job of identifying the correct usage of the word **\"house\"** to mean **\"a gambling house\"**. It most likely uses the context from **\"wins\"** to determine the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('house.n.01') - a dwelling that serves as living quarters for one or more families\n",
      "Synset('firm.n.01') - the members of a business organization that owns or operates one or more establishments\n",
      "Synset('house.n.03') - the members of a religious community living together\n",
      "Synset('house.n.04') - the audience gathered together in a theatre or cinema\n",
      "Synset('house.n.05') - an official assembly having legislative powers\n",
      "Synset('house.n.06') - aristocratic family line\n",
      "Synset('house.n.07') - play in which children take the roles of father or mother or children and pretend to interact like adults\n",
      "Synset('sign_of_the_zodiac.n.01') - (astrology) one of 12 equal areas into which the zodiac is divided\n",
      "Synset('house.n.09') - the management of a gambling house or casino\n",
      "Synset('family.n.01') - a social unit living together\n",
      "Synset('theater.n.01') - a building where theatrical performances or motion-picture shows can be presented\n",
      "Synset('house.n.12') - a building in which something is sheltered or located\n",
      "Synset('house.v.01') - contain or cover\n",
      "Synset('house.v.02') - provide housing for\n",
      "\n",
      "Context Sentence: They say the house always wins. \n",
      "\n",
      "Target Word: house \n",
      "\n",
      "Synset('house.n.09') - the management of a gambling house or casino\n"
     ]
    }
   ],
   "source": [
    "# lesk parameters\n",
    "contSent = ['They', 'say', 'the', 'house', 'always', 'wins', '.']\n",
    "tarWord = 'house'\n",
    "\n",
    "# Alternate parameters that can be run by uncommenting the below variables\n",
    "# contSent = ['The', 'Lucchese', 'family', 'was', 'involved', 'in', 'several', 'crimes', '.']\n",
    "# tarWord = 'family'\n",
    "\n",
    "# displays all synsets of the chosen word\n",
    "for syn in wn.synsets(tarWord):\n",
    "    print(syn, '-', syn.definition())\n",
    "print()\n",
    "\n",
    "leskVal = lesk(contSent, tarWord)\n",
    "\n",
    "# Formats the context sentence list into an actual sentence string for cleaner display\n",
    "sentence = ''\n",
    "for currWord in contSent:\n",
    "    if contSent.index(currWord) < len(contSent) - 2:\n",
    "        sentence = sentence + currWord + ' '\n",
    "    else:\n",
    "        sentence = sentence + currWord\n",
    "\n",
    "# Outputs formatted results\n",
    "print('Context Sentence:', sentence, '\\n')\n",
    "print('Target Word:', tarWord, '\\n')\n",
    "# Displays the computed synset and definition of the target word used in the context of the sentence\n",
    "print(leskVal, '-', leskVal.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameters below, however, we can see that the algorithm has its limits. Here, the word **\"car\"** is identified to be most similar to `car.n.04` (**\"where passengers ride up and down\"**), whereas my goal was to use **\"car\"** to mean **\"cable car\"**. While it is seen that `cable_car.n.01` is a synset for **\"car\"** and the sentence even includes the word **\"cable\"** for additional context, `lesk()` identifies the usage to be `car.n.04`. I believe this is most likely due to the word **\"up\"** being closer to the target word than the word **\"cable\"**. While this result is not technically incorrect, it shows that the Lesk algorithm does not always use the context in the sentence as well as a human does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('car.n.01') - a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Synset('car.n.02') - a wheeled vehicle adapted to the rails of railroad\n",
      "Synset('car.n.03') - the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "Synset('car.n.04') - where passengers ride up and down\n",
      "Synset('cable_car.n.01') - a conveyance for passengers or freight on a cable railway\n",
      "\n",
      "Context Sentence: They rode the car up the cable. \n",
      "\n",
      "Target Word: car \n",
      "\n",
      "Synset('car.n.04') - where passengers ride up and down\n"
     ]
    }
   ],
   "source": [
    "# Alternate parameters\n",
    "contSent = ['They', 'rode', 'the', 'car', 'up', 'the', 'cable', '.']\n",
    "tarWord = 'car'\n",
    "\n",
    "# displays all synsets of the chosen word\n",
    "for syn in wn.synsets(tarWord):\n",
    "    print(syn, '-', syn.definition())\n",
    "print()\n",
    "\n",
    "leskVal = lesk(contSent, tarWord)\n",
    "\n",
    "# Formats the context sentence list into an actual sentence string for cleaner display\n",
    "sentence = ''\n",
    "for currWord in contSent:\n",
    "    if contSent.index(currWord) < len(contSent) - 2:\n",
    "        sentence = sentence + currWord + ' '\n",
    "    else:\n",
    "        sentence = sentence + currWord\n",
    "\n",
    "# Outputs formatted results\n",
    "print('Context Sentence:', sentence, '\\n')\n",
    "print('Target Word:', tarWord, '\\n')\n",
    "# Displays the computed synset and definition of the target word used in the context of the sentence\n",
    "print(leskVal, '-', leskVal.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordnet\n",
    "\n",
    "SentiWordNet is a lexical analysis tool that allows sentiment analysis. For a synset from WordNet, SentiWordNet has assigned a set of sentiment scores: positivity, negativity, and objectivity. By downloading the `sentiwordnet` corpus, NLTK can be used to utilzie SentiWordNet. By using WordNet to determine the usages of the words in a piece of text, SentiWordNet can be used to determine that text's sentiments. This can be a useful tool for softwares to understanding texts better than just determining the meanings of all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding SentiSynsets For a Word\n",
    "\n",
    "`senti_synsets(word, pos)`\n",
    "\n",
    "`word`: String - The chosen word\n",
    "\n",
    "`pos`: *(Optional)* String - Part of speech constraint\n",
    "\n",
    "*returns*: Filter **(Must be Typecast to `list`)** - The sentisynsets for the chosen word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hideous.s.01's Scores:\tPositivity:\t 0.0\n",
      "\t\t\tNegativity:\t 0.875\n",
      "\t\t\tObjectivity:\t 0.125 \n",
      "\n",
      "awful.s.02's Scores:\tPositivity:\t 0.0\n",
      "\t\t\tNegativity:\t 0.625\n",
      "\t\t\tObjectivity:\t 0.375 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = 'horrific'\n",
    "sentiSynList = list(swn.senti_synsets(word))\n",
    "\n",
    "# Displays all sentisynsets for word and their scores\n",
    "for sSyn in sentiSynList:\n",
    "    # Formatted Output for current sentisynset\n",
    "    print('%s\\'s Scores:\\tPositivity:\\t' % sSyn.synset.name(), sSyn.pos_score())\n",
    "    print('\\t\\t\\tNegativity:\\t', sSyn.neg_score())\n",
    "    print('\\t\\t\\tObjectivity:\\t', sSyn.obj_score(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Polarity Scores For Sentences\n",
    "\n",
    "The example sentence is a very positive sentence, which is reflected in the high positivity score. Curiously enough, however, there is also a negative score on this sentence when to the human eye there is no negativity present. In order to show where this may come from, the sentisynsets generated for the tokens are also logged. This shows some curious sentisynsets being tagged. The word \"I\", referring to the speaker, gets tagged as `iodine.n.01`. Similarly, \"it\", referring to the restaurant, gets tagged as `information_technology.n.01`. While neither of these explicitly affect the results of the polarity calculation, they show the potential issues with using SentiWordNet without additional processing. The negativity comes from the word \"is\", which is tagged as `be.v.01`, which, according to SentiWordNet, has a slightly negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - <iodine.n.01: PosScore=0.0 NegScore=0.0>\n",
      "love - <love.n.01: PosScore=0.625 NegScore=0.0>\n",
      "restaurant - <restaurant.n.01: PosScore=0.0 NegScore=0.0>\n",
      "it - <information_technology.n.01: PosScore=0.0 NegScore=0.0>\n",
      "is - <be.v.01: PosScore=0.25 NegScore=0.125>\n",
      "delicious - <delicious.n.01: PosScore=0.0 NegScore=0.0>\n",
      "\n",
      "Sentence: I love that restaurant it is delicious\n",
      "Polarity scores:\tPositive: 0.875\n",
      "\t\t\tNegative: 0.125\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love that restaurant it is delicious'\n",
    "# Processes the sentence into a list of string tokens that each contain a word from the sentence\n",
    "tokens = sentence.split()\n",
    "\n",
    "posScore = 0\n",
    "negScore = 0\n",
    "\n",
    "for currTok in tokens:\n",
    "    currSynList = list(swn.senti_synsets(currTok))\n",
    "    if currSynList:\n",
    "        # Extracts a sentisynset since not all in the list will be necessary\n",
    "        sSyn = currSynList[0]\n",
    "        negScore += sSyn.neg_score()\n",
    "        posScore += sSyn.pos_score()\n",
    "\n",
    "        #Logging\n",
    "        print(currTok, '-', sSyn)\n",
    "\n",
    "print('\\nSentence:', sentence)\n",
    "print('Polarity scores:\\tPositive:', posScore)\n",
    "print('\\t\\t\\tNegative:', negScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Collocations are combinations of words that have different or more meaning than any of their words individually do. For example, **\"hot dog\"** has a different meaning than either of its two parts. Such collocations appear often enough that they can usually be detected. Since the chance of the unrelated words being in that specific order are lower than the frequency of the collocation occurring, it can usually be assumed that these combinations of words have the alternate meaning when they are found. Recognizing these common meanings can be useful for language processing since normal human speech uses many such groupings of words.\n",
    "\n",
    "`text.collocations()`\n",
    "\n",
    "`text`: NLTK Text object - The text object that is being searched to find collocations\n",
    "\n",
    "*returns*: The collocations found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n"
     ]
    }
   ],
   "source": [
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "Mutual information is a measure of of the mutual dependence between two random variables. The mutual information of a collocation is the measure of how likely it is that the second word of a collocation will appear if the first word has already been found. This can be calculated with the formula: `MI(word1, word2) = log(P(word1, word2)/(P(word1)*P(word2)))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I chose to calculate the mutual information of the collocation **\"fellow citizens\"**. One thing I noticed was that NLTK identified **\"fellow citizens\"** and **\"Fellow citizens\"** as two separate collocations due to the capitalization. To get around this, I made the full text lowercase. A mutual information measurement of 4.13206 is fairly high compared to any set of two words that don't create a new meaning, indicating that this word combination is a collocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information of \"fellow citizens\": 4.13206\n"
     ]
    }
   ],
   "source": [
    "# Designed so only collocation variable needs to be changed\n",
    "bothWords = 'fellow citizens'\n",
    "word1,word2 = bothWords.split(' ')\n",
    "\n",
    "# The tokens are made into a single string so consecutive words can be searched for\n",
    "# lowercase adjusts for any differences in capitalization during the counting process\n",
    "fullText = ' '.join(text4.tokens).lower()\n",
    "\n",
    "# determines number of unique tokens in the text\n",
    "numUniToks = len(set(text4))\n",
    "pBothWords = fullText.count(bothWords) / numUniToks\n",
    "pWord1 = fullText.count(word1) / numUniToks\n",
    "pWord2 = fullText.count(word2) / numUniToks\n",
    "\n",
    "mutInfo = math.log2(pBothWords / (pWord1 * pWord2))\n",
    "\n",
    "print('Mutual information of \\\"%s\\\": %1.5f' % (bothWords, mutInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, the mutual information of the non-collocation **\"the one\"** is significantly lower than that of the collocation above. In fact, a negative MI measurement can indicate that the occurence of the two words in this order is even lower in this text than would be possible by random chance. The clear disparity in the MI values of the two word combinations shows that collocations are so common that when those combinations of words are found, it is reasonable to assume and interpret them with the meaning of the collocation instead of the individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information of \"the one\": -5.45581\n"
     ]
    }
   ],
   "source": [
    "# two unrelated words next to each other\n",
    "bothWords = 'the one'\n",
    "word1,word2 = bothWords.split(' ')\n",
    "\n",
    "# The tokens are made into a single string so consecutive words can be searched for\n",
    "# lowercase adjusts for any differences in capitalization during the counting process\n",
    "fullText = ' '.join(text4.tokens).lower()\n",
    "\n",
    "# determines number of unique tokens in the text\n",
    "numUniToks = len(set(text4))\n",
    "pBothWords = fullText.count(bothWords) / numUniToks\n",
    "pWord1 = fullText.count(word1) / numUniToks\n",
    "pWord2 = fullText.count(word2) / numUniToks\n",
    "\n",
    "mutInfo = math.log2(pBothWords / (pWord1 * pWord2))\n",
    "\n",
    "print('Mutual information of \\\"%s\\\": %1.5f' % (bothWords, mutInfo))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
