{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "I previously explored using various ML model techniques to classify text. This notebook continues to explore various approaches to Text Classification using the Keras Deep Learning API. Using the dataset `spam_assassin_csv`, I evaluated the performance of various model architectures in classifying an email as spam or not based on its contents. The reason I chose this dataset is because I've always been curious how real email services determine spam. By learning more about some deep learning applications in text classification I want to better understand how the real thing works.\n",
    "\n",
    "*Note*: Some of this code is inspired by the sample code provided on Dr. Mazidi's Github. I used her code to guide my exploration of how to use Keras. Also, my laptop is quite weak and really struggled with fitting these models. It regularly ran out of RAM and took hours to run even a few epochs so that is why some of the models were not able to finish fitting. I had to stop running them because my laptop could not handle the load. I did my best to work around these issues where possible, but my laptop's performance was a serious bottleneck to my work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For data plotting\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To split the given dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For applying ML models\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, Input, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "# for evaluating model performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...       0\n",
      "1  From gort44@excite.com Mon Jun 24 17:54:21 200...       1\n",
      "2  From fork-admin@xent.com Mon Jul 29 11:39:57 2...       1\n",
      "3  From dcm123@btamail.net.cn Mon Jun 24 17:49:23...       1\n",
      "4  From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...       0\n"
     ]
    }
   ],
   "source": [
    "# parses  data into dataframe\n",
    "data = pd.read_csv('spam_assassin.csv')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape:\t (4636, 2)\n",
      "Testing Dataset Shape:\t (1160, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splits data into 80% training, 20% testing\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=1234)\n",
    "print('Training Dataset Shape:\\t', train.shape)\n",
    "print('Testing Dataset Shape:\\t', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Distributions\n",
    "\n",
    "The data set has a total of 5796 data points. Using the sklearn `train_test_split()` method the dataset is split into train and test sets. Based on the distributions seen in the images saved, the sets have approximately double the number of non-spam emails as spam. Trained using these sets, the model should be able to predict whether a test data sample should have a spam (1) or non-spam(0) classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainPlot = sb.countplot(y = train['target'])\n",
    "trainPlot.get_figure().savefig('trainPlot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOFElEQVR4nO3dfaxkdX3H8feXXRdkoSAPEsqCF4yBQiSCW4ViiKitQBRNQxMIDba1pT70QUk0kBoa2zRNtWksiakFS6EE0IpWCEWpBaoNteBdZGEBkVUWWARXIOCCjfLw7R/nd5fhurt32rnnzuG771dys+f8ZnbO594587lnfjP3TGQmkqR6dpp2AElSPyx4SSrKgpekoix4SSrKgpekopZPO8CoffbZJ2dmZqYdQ5JeMtasWfNoZu67tcsGVfAzMzPMzs5OO4YkvWRExP3buswpGkkqyoKXpKIseEkqyoKXpKIseEkqyoKXpKIseEkqyoKXpKIseEkqalB/yXr3xsd4/Uf+adoxJGnJrPnkmb3dtkfwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklSUBS9JRfVa8BFxYkTcExHrI+KcPrclSXqx3go+IpYBnwZOAg4HTo+Iw/vaniTpxfo8gn8DsD4zv5+ZPwM+B7yrx+1Jkkb0WfAHAA+OrG9sYy8SEWdFxGxEzD77k809xpGkHcvUX2TNzAsyc3Vmrl6+6+7TjiNJZfRZ8A8BB46sr2pjkqQl0GfBfwt4TUQcHBErgNOAq3vcniRpxPK+bjgzn42IPwCuA5YBF2XmnX1tT5L0Yr0VPEBmXgtc2+c2JElbN/UXWSVJ/bDgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16SirLgJakoC16Silqw4CPi4HHGJEnDMs4R/Be3MnblYgeRJC2u5du6ICIOA44A9oiIXx+56BeAXfoI80ur9mb2k2f2cdOStMPZZsEDhwLvAPYE3jkyvhn4vR4zSZIWwTYLPjOvAq6KiGMz85tLmEmStAjGmYN/LCKuj4h1ABFxZER8rOdckqQJjVPwFwLnAs8AZObtwGl9hpIkTW6cgt81M2+ZN/ZsH2EkSYtnnIJ/NCJeDSRARJwKPNxrKknSxLb3Lpo5HwQuAA6LiIeA+4Df7DWVJGliCxZ8Zn4feFtErAR2yszN/ceSJE1qwYKPiLPnrQM8CazJzNv6iSVJmtQ4c/CrgfcBB7Sv3wdOBC6MiI/2mE2SNIFx5uBXAUdn5lMAEfGnwL8CxwNrgE/0F0+S9P81zhH8K4Gfjqw/A+yXmf8zb1ySNCDjHMFfBtwcEVe19XcCl7cXXe/qLZkkaSLbLfjoXlG9GPgKcFwbfl9mzrblM/qLJkmaxHYLPjMzIq7NzNcCs9u7riRpWMaZg781In659ySSpEU1zhz8G4EzIuJ+4Gkg6A7uj+w1mSRpIuMU/Nt7TyFJWnTjnKrgfoCIeCU9fVSfJGnxLTgHHxGnRMS9dCcZ+zqwge5dNZKkARvnRdY/B44BvpuZBwNvBf6711SSpImNMwf/TGY+FhE7RcROmXljRHyqjzA/e/hOHviz1/Zx09qBHXTeHdOOIE3FOAX/RETsBnwDuCwiNgFP9RtLkjSpcQp+LfAT4MN0f7m6B7Bbn6EkSZMbp+BPyMzngeeBSwAi4vZeU0mSJrbNgo+I9wMfAF49r9B3B27qO5gkaTLbO4K/nO7tkH8JnDMyvjkzH+81lSRpYtss+Mx8ku6j+U5fujiSpMUyzvvgJUkvQRa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSURa8JBVlwUtSUb0VfERcFBGbImJdX9uQJG1bn0fwFwMn9nj7kqTt6K3gM/MbwON93b4kafumPgcfEWdFxGxEzD7+9HPTjiNJZUy94DPzgsxcnZmr91q5bNpxJKmMqRe8JKkfFrwkFdXn2ySvAL4JHBoRGyPivX1tS5L085b3dcOZeXpfty1JWphTNJJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUVZ8JJUlAUvSUUtn3aAUSv2P4KDzpuddgxJKsEjeEkqyoKXpKIseEkqyoKXpKIseEkqyoKXpKIseEkqyoKXpKIseEkqyoKXpKIiM6edYYuI2AzcM+0c27EP8Oi0Q2zH0PPB8DMOPR8MP6P5Jvd/yfiqzNx3axcM6lw0wD2ZuXraIbYlImbNN5mhZxx6Phh+RvNNbrEyOkUjSUVZ8JJU1NAK/oJpB1iA+SY39IxDzwfDz2i+yS1KxkG9yCpJWjxDO4KXJC0SC16SihpEwUfEiRFxT0Ssj4hzppjjoojYFBHrRsb2ioivRcS97d9XtPGIiPNb5tsj4uglyHdgRNwYEXdFxJ0R8cdDyhgRu0TELRGxtuX7eBs/OCJubjk+HxEr2vjObX19u3ymz3wjOZdFxLcj4pqB5tsQEXdExG0RMdvGBnEft23uGRFXRsR3IuLuiDh2YPkObT+7ua8fR8SHBpbxw+0xsi4irmiPncXfDzNzql/AMuB7wCHACmAtcPiUshwPHA2sGxn7BHBOWz4H+Ku2fDLwFSCAY4CblyDf/sDRbXl34LvA4UPJ2LazW1t+GXBz2+4/A6e18c8A72/LHwA+05ZPAz6/RPfz2cDlwDVtfWj5NgD7zBsbxH3ctnkJ8LtteQWw55Dyzcu6DHgEeNVQMgIHAPcBLx/Z/36rj/1wyX7Q2/lmjwWuG1k/Fzh3inlmeHHB3wPs35b3p/tjLIC/B07f2vWWMOtVwK8OMSOwK3Ar8Ea6v8hbPv/+Bq4Djm3Ly9v1oudcq4DrgbcA17QH9WDytW1t4OcLfhD3MbBHK6cYYr6t5P014KYhZaQr+AeBvdp+dQ3w9j72wyFM0cx9s3M2trGh2C8zH27LjwD7teWp5m5P046iO0oeTMY2/XEbsAn4Gt2zsycy89mtZNiSr13+JLB3n/mATwEfBZ5v63sPLB9AAv8WEWsi4qw2NpT7+GDgR8A/tmmuz0bEygHlm+804Iq2PIiMmfkQ8NfAA8DDdPvVGnrYD4dQ8C8Z2f0Knfr7SiNiN+CLwIcy88ejl007Y2Y+l5mvoztSfgNw2LSyzBcR7wA2ZeaaaWdZwJsy82jgJOCDEXH86IVTvo+X001j/l1mHgU8TTfdscW098E5bQ77FOAL8y+bZsY29/8uul+WvwisBE7sY1tDKPiHgANH1le1saH4YUTsD9D+3dTGp5I7Il5GV+6XZeaXhpgRIDOfAG6ke6q5Z0TMnfdoNMOWfO3yPYDHeox1HHBKRGwAPkc3TfO3A8oHbDnCIzM3Af9C94tyKPfxRmBjZt7c1q+kK/yh5Bt1EnBrZv6wrQ8l49uA+zLzR5n5DPAlun1z0ffDIRT8t4DXtFeQV9A9pbp6yplGXQ28py2/h27ee278zPYK/DHAkyNP/3oREQH8A3B3Zv7N0DJGxL4RsWdbfjnd6wN30xX9qdvIN5f7VOCGdmTVi8w8NzNXZeYM3X52Q2aeMZR8ABGxMiJ2n1umm0Nex0Du48x8BHgwIg5tQ28F7hpKvnlO54XpmbksQ8j4AHBMROzaHtNzP8PF3w+X6sWOBV50OJnuHSHfA/5kijmuoJsTe4buSOW9dHNd1wP3Av8O7NWuG8CnW+Y7gNVLkO9NdE8rbwdua18nDyUjcCTw7ZZvHXBeGz8EuAVYT/d0eec2vktbX98uP2QJ7+s388K7aAaTr2VZ277unHs8DOU+btt8HTDb7ucvA68YUr623ZV0R7l7jIwNJiPwceA77XFyKbBzH/uhpyqQpKKGMEUjSeqBBS9JRVnwklSUBS9JRVnwklSUBS8tonbWwl2nnUMCP9FJWlTtr2RXZ+aj084ieQSvHU5EnNnO+702Ii6NiJmIuKGNXR8RB7XrXRwRp478v6fav2+OiP+IF86Jfln7K8g/oju3yI0RceN0vjvpBcsXvopUR0QcAXwM+JXMfDQi9qI7v/klmXlJRPwOcD7w7gVu6ijgCOAHwE3AcZl5fkScDZzgEbyGwCN47WjeAnxhroAz83G6E6Jd3i6/lO6UEAu5JTM3ZubzdKeMmFn8qNJkLHhp256lPUYiYie6Ty+a89OR5efw2bAGyILXjuYG4DciYm/oPusU+C+6s0sCnAH8Z1veALy+LZ9C9zGEC9lM93GK0tR51KEdSmbeGRF/AXw9Ip6jO/vlH9J9QtFH6D6t6Lfb1S8EroqItcBX6T7cYiEXAF+NiB9k5gmL/x1I4/NtkpJUlFM0klSUBS9JRVnwklSUBS9JRVnwklSUBS9JRVnwklTU/wL1UyDa3p5GhAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testPlot = sb.countplot(y = test['target'])\n",
    "trainPlot.get_figure().savefig('testPlot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes:\t (4636, 25000) \t (4636,)\n",
      "test shapes:\t (1160, 25000) \t (1160,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "numClasses = 2\n",
    "maxToks = 25000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = maxToks)\n",
    "tokenizer.fit_on_texts(train.text)\n",
    "\n",
    "trainX = tokenizer.texts_to_matrix(train.text, mode='tfidf')\n",
    "testX = tokenizer.texts_to_matrix(test.text, mode='tfidf')\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(train.target)\n",
    "trainY = labelEncoder.transform(train.target)\n",
    "testY = labelEncoder.transform(test.target)\n",
    "\n",
    "print('train shapes:\\t', trainX.shape, '\\t', trainY.shape)\n",
    "print('test shapes:\\t', testX.shape, '\\t', testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "38/38 [==============================] - 2s 35ms/step - loss: 0.3444 - accuracy: 0.8511 - val_loss: 0.0993 - val_accuracy: 0.9763\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.0442 - accuracy: 0.9941 - val_loss: 0.0269 - val_accuracy: 0.9935\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0130 - accuracy: 0.9992 - val_loss: 0.0158 - val_accuracy: 0.9978\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.0067 - accuracy: 0.9992 - val_loss: 0.0113 - val_accuracy: 0.9978\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.0041 - accuracy: 0.9997 - val_loss: 0.0092 - val_accuracy: 0.9989\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.9989\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 0.9989\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 0.9978\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9978\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 1s 21ms/step - loss: 9.1928e-04 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 0.9989\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 1s 28ms/step - loss: 7.5724e-04 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 0.9989\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 6.3457e-04 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9989\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 5.4316e-04 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 0.9989\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 4.7183e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9989\n",
      "Epoch 15/20\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 4.1350e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9989\n",
      "Epoch 16/20\n",
      "38/38 [==============================] - 1s 20ms/step - loss: 3.6539e-04 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9989\n",
      "Epoch 17/20\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 3.2571e-04 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9989\n",
      "Epoch 18/20\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 2.9235e-04 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9989\n",
      "Epoch 19/20\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 2.6438e-04 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9989\n",
      "Epoch 20/20\n",
      "38/38 [==============================] - 1s 17ms/step - loss: 2.3985e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "batchSize = 100\n",
    "\n",
    "seqModel = models.Sequential()\n",
    "seqModel.add(layers.Dense(12, input_dim=maxToks, kernel_initializer='normal', activation='relu'))\n",
    "seqModel.add(layers.Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "seqModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "history = seqModel.fit(trainX, trainY, batch_size=batchSize, epochs=20, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0088 - accuracy: 0.9966\n",
      "Test Accuracy:  0.9965517520904541\n"
     ]
    }
   ],
   "source": [
    "score = seqModel.evaluate(testX, testY, batch_size=batchSize, verbose=1)\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "This model clearly worked very well, with its incredibly high accuracy on the test dataset. This is obviously not a sure indicator of its performance on other data, but does show great promise for Sequential models ran on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Since this is a binary classification dataset, the model's predicitons need to be converted to 0s or 1s. To do this I round them to 1 if the classification is > 0.5 and to 0 if it is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = seqModel.predict(testX)\n",
    "labelPreds = [1 if curr > 0.5 else 0 for curr in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score:\t99.461%\n",
      "Accuracy Score:\t\t99.655%\n",
      "Recall Score:\t\t99.461%\n",
      "F1 Score:\t\t99.461%\n"
     ]
    }
   ],
   "source": [
    "print('Precision Score:\\t%.3f%%' % (precision_score(testY, labelPreds) * 100))\n",
    "print('Accuracy Score:\\t\\t%.3f%%' % (accuracy_score(testY, labelPreds) * 100))\n",
    "print('Recall Score:\\t\\t%.3f%%' % (recall_score(testY, labelPreds) * 100))\n",
    "print('F1 Score:\\t\\t%.3f%%' % (f1_score(testY, labelPreds) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxToks = 25000\n",
    "\n",
    "rnnModel = models.Sequential()\n",
    "rnnModel.add(layers.Embedding(maxToks, 32))\n",
    "rnnModel.add(layers.SimpleRNN(32))\n",
    "rnnModel.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "38/38 [==============================] - 2097s 55s/step - loss: 0.6357 - accuracy: 0.6753 - val_loss: 0.6462 - val_accuracy: 0.6541\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 2068s 54s/step - loss: 0.6323 - accuracy: 0.6753 - val_loss: 0.6449 - val_accuracy: 0.6541\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 2068s 54s/step - loss: 0.6314 - accuracy: 0.6753 - val_loss: 0.6462 - val_accuracy: 0.6541\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 2071s 55s/step - loss: 0.6310 - accuracy: 0.6753 - val_loss: 0.6452 - val_accuracy: 0.6541\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 2071s 54s/step - loss: 0.6317 - accuracy: 0.6753 - val_loss: 0.6526 - val_accuracy: 0.6541\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 2070s 54s/step - loss: 0.6313 - accuracy: 0.6753 - val_loss: 0.6499 - val_accuracy: 0.6541\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 2071s 55s/step - loss: 0.6313 - accuracy: 0.6753 - val_loss: 0.6449 - val_accuracy: 0.6541\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 2084s 55s/step - loss: 0.6313 - accuracy: 0.6753 - val_loss: 0.6466 - val_accuracy: 0.6541\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 2076s 55s/step - loss: 0.6312 - accuracy: 0.6753 - val_loss: 0.6482 - val_accuracy: 0.6541\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 2075s 55s/step - loss: 0.6307 - accuracy: 0.6753 - val_loss: 0.6454 - val_accuracy: 0.6541\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 2076s 55s/step - loss: 0.6308 - accuracy: 0.6753 - val_loss: 0.6450 - val_accuracy: 0.6541\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 2071s 54s/step - loss: 0.6311 - accuracy: 0.6753 - val_loss: 0.6477 - val_accuracy: 0.6541\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 2076s 55s/step - loss: 0.6310 - accuracy: 0.6753 - val_loss: 0.6467 - val_accuracy: 0.6541\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 2075s 55s/step - loss: 0.6313 - accuracy: 0.6753 - val_loss: 0.6462 - val_accuracy: 0.6541\n",
      "Epoch 15/20\n",
      "17/38 [============>.................] - ETA: 18:58 - loss: 0.6318 - accuracy: 0.6741"
     ]
    }
   ],
   "source": [
    "batchSize = 100\n",
    "rnnModel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "history = rnnModel.fit(trainX, trainY, epochs=20, batch_size=batchSize, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this model did not finish trianing for the full 20 epochs, the validation accuracy plateaued at 65.41%. This means further training of the model is not really necessary since this is approximately the lowest the loss can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rnnModel.predict(testX)\n",
    "labelPreds = [1 if curr > 0.5 else 0 for curr in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision Score:\\t%.3f%%' % (precision_score(testY, labelPreds) * 100))\n",
    "print('Accuracy Score:\\t\\t%.3f%%' % (accuracy_score(testY, labelPreds) * 100))\n",
    "print('Recall Score:\\t\\t%.3f%%' % (recall_score(testY, labelPreds) * 100))\n",
    "print('F1 Score:\\t\\t%.3f%%' % (f1_score(testY, labelPreds) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since the model was interrupted and my laptop is struggling with running the model, there were some issues with evaluating the model on the test data set. But by looking at the performance of the model on the validation dataset, it can be seen that this RNN model was significantly less accurate than the Sequential model. This model has a 65.41% accuracy on the validation set while the Sequential model had a 99.89% acccuracy on its validation set. Although there is no guarantee that the validation data is roughly indicative of the test data, in this case, since the train, validation, and test data sets are chosen randomly from the same uniform distribution of data, we can look at the performance on the validation data set as a fair indicator of the model's performance.\n",
    "\n",
    "#### Conclusion\n",
    "It is fairly clear that the RNN model is less accurate than the Sequential model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "textDataset = tf.data.Dataset.from_tensor_slices(train['text'].to_numpy()).batch(128)\n",
    "vectorizer.adapt(textDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "wordInd = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "embeddingLayer = layers.Embedding(len(wordInd) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         2560128   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,740,994\n",
      "Trainable params: 2,740,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# add more layers\n",
    "intSeqInput = Input(shape=(None,), dtype=\"int64\")\n",
    "embeddedSeqs = embeddingLayer(intSeqInput)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embeddedSeqs)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(numClasses, activation=\"softmax\")(x)\n",
    "embModel = Model(intSeqInput, preds)\n",
    "embModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 1254s 43s/step - loss: 0.5821 - acc: 0.6845 - val_loss: 0.4213 - val_acc: 0.8481\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1199s 41s/step - loss: 0.3605 - acc: 0.8657 - val_loss: 0.2870 - val_acc: 0.9235\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1256s 43s/step - loss: 0.2324 - acc: 0.9194 - val_loss: 0.1821 - val_acc: 0.9386\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1394s 48s/step - loss: 0.2110 - acc: 0.9229 - val_loss: 0.1657 - val_acc: 0.9450\n",
      "Epoch 5/10\n"
     ]
    }
   ],
   "source": [
    "batchSize = 128\n",
    "embModel.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "embModel.fit(trainX, trainY, batch_size=batchSize, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this model did not finish trianing for the full 20 epochs, the validation accuracy was approaching >90% and steadily increasing. Based on this, I decided to stop the model since it showed that this model was quite accurate on the data.\n",
    "\n",
    "In this case, since the model was interrupted and my laptop is struggling with running the model, there were again some issues with evaluating the model on the test data set. Compared to the RNN model, this model using an embedding layer was significantly more accurate. The one downside of stopping the model fitting early is that I don't know for sure whether this model could get more or less ccurate than the first Sequential model. Although there is no guarantee that the validation data is roughly indicative of the test data, in this case, since the performance on the validation set is atill quite high (95.5% as last seen), it can be assumed that this is a helpful technique.\n",
    "\n",
    "#### Conclusion\n",
    "It is fairly clear that this model is more accurate than the RNN model on this dataset but it is unclear if the model with an embedding layer will be more accurate than the first Sequential model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion\n",
    "\n",
    "Unfortunately, the hardware that is avaliable to me is severely underpowered for the tasks I was trying to evaluate for this project. This ccaused me to have to stop some of the models' fitting processes before they were completed. However, even despite being able to get concrete test data performance statistics on some of these models, I was able to get a decent look at their performance. The Sequential model performed leagues better than the RNN model, and it is unclear whether the model with embedding would have outperformed the Sequential model but it definitely outperformed the RNN model. Overall, this project gave me another look into using existing ML tools for realistic text classification problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
